# Studying

- RoPE: encodes the rotation distance between the Key and Query for
additional context to the attention mechanism both in the relative
distance and different rates of rotation for each of the hidden dimensions of 
the vector.
- "The theme that AI won't take your job but somebody using AI will."
- Don't sit around hypothesizing what AI could do, wake up every day and applying LLMs to my organization.
- "So much of the success I have seen on the topic of successfull LLM integrations is a question of scale. It is totally possible to use a shovel for some digging jobs but 
at other times you want a front end loader. It is not appropriate when dealing with a lot data to individually
copy and paste data into chatgpt. This is why learning how to code is so important to wrap your ideas into applications
and applications into pipelines that leverages some of the analytics prowess of large language models." (https://www.youtube.com/watch?v=-DqG0LZxzFw)
- A funny part of this is he then recommends using AWS and Azure as a cost effective way to use LLMs on a pay as you go model.
- Excellent explanation of RoPE with the math (https://www.youtube.com/watch?v=2tS_bXPoriI)
- A explainer of LongRoPE which is almost certainly how Gemini and others use super long context windows (https://www.youtube.com/watch?v=RCSvpYb90qE)
- Boosting LLM/RAG Workflows and details about composable memory and checkpointing (https://www.youtube.com/watch?v=ccaDEFoKwko)
- ["We Can All Be AI Engineers and We Can Do It with Open Source Models // Luke Marsden // Podcast #273
"](https://www.youtube.com/watch?v=6BBm9DlE78g) "RAG is such a bad term what you mean is Knowledge."
  - "Writing AI applications should be declarative, reproducable, and design and could even be deployable by non technical staff
  such as the GPT model from OpenAI."
