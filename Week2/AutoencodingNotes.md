# Autoencoding notes
## [[Mastering transformers]]


- Bert was one of the first autoencoder Transformer stack
- The language model after pretreaining is able to provide
a global understanding of the language it is trained on.
- Bert uses WordPiece tokenization. 
- The purpose of tokenizers for transformers and other subtoken architectures 
are to help the model deal with unknown tokens.
- Bert also uses positional encoding to ensure the position 
of the tokens 
