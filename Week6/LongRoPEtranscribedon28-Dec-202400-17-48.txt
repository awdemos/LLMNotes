So today, we got a paper for you guys called Long Rope. So this is a kind of a remix on a remix on a niche concept. So it's going to take a little second for me to unpack exactly what's going on here. But people are hyped about this. And why are people hyped about this? So if you saw some of the streams that I think was last week's streams or the week before that, but we were basically looking at Gemini 1.5, which is Google's paper. And Gemini 1.5 basically has state of the art context length, which is how much information you can feed into a transformer that it can attend to at the same time. We don't know what they did, right? So this is a comment on a comment of a interview with Demis Hassabis, which is this guy here. This is the head of DeepMind. And he basically said in an interview recently that he admits that there is some secret sauce and how Gemini is able to process a 10 million token context window. The extreme context length of Gemini 1.5 Pro can't be achieved without some new innovations. So something, the people at DeepMind discovered something that allowed them to basically 10x the context of a transformer, which is huge, right? Because transformers are behind all your favorite things, right? They're behind video understanding models now. They're behind diffusion models now. They're behind language models. They're basically part of everything. So if you can find some novel architectural or little hack that allows you to basically drastically increase the length of the input or the context, then that's a pretty significant advantage. So Google figured out how to do this, and they're not telling anybody, right? So in the stream where we reviewed Gemini 1.5, I hypothesized that maybe it's ring attention. Could be possible with like a TPU Taurus topology, which is a terminology that was introduced in the ring attention paper. But basically ring attention works by kind of, you could think of it like there, it's kind of like this rolling computation where you're kind of passing forward KVCaches to this little ring of GPUs. And it allows you to have a very long context. So that's what I hypothesized at that time. But recently this past week, I saw all the hype around this long rope, and after reading this long rope paper, I feel like this might actually be what they're doing, right? So now I've kind of shifted my hypothesis. I think maybe what they discovered is they discovered something similar to this, right? Not exactly long rope, but something within this kind of sphere that would allow them to basically extend the context length of transformers. And the reason I think this one's pretty good is because this one is almost like you can do it separately. It's like orthogonal to other types of things. So models extended via long rope retain the original architecture with minor modifications to the positional embedding. So basically this is just a little trick that you can do with positional embeddings that will allow you to take a transformer that can be used for anything. You don't have to change it much, it can be the same transformer. And then you add this long rope positional embeddings. And now it has a much longer context. So it could be what's behind Gemini 1.5, but it could also not be behind what's Gemini 1.5, right? So maybe some of you watching work at Google and you're looking at this right now and you're like, wow, he's a complete idiot. He has no idea. We're not using long rope at all. It's totally this. But I don't work at Google anymore. So I have no idea what they're doing. So this is just my best guess. Okay. So what does long rope stand for? So long rope, as I said before, it's a remix on a remix. So the PE part of this long rope stands for position embeddings. So position embeddings. The way I usually introduce these in other streams here, for example, is a transformer that's consuming a image, normally called a VIT or a vision transformer. But when you feed an image into a transformer like this, you have to turn it into a sequence, right? A transformer is a sequence to sequence model and needs a sequence of things or a list of things. And here we're turning this image of these two mushrooms into this list of visual patches, sometimes also called visual tokens. And you can see that we're not just giving the transformer a list of these visual tokens. We're also adding this little position here, right? So these little purple squares that have 0, 1, 2, 3, 4, 5, those are the position embeddings or PE, right? So that's the PE, and that's the original position embedding concept, right? Is little vectors that basically tell the transformer where in the sequence this individual patch or token is, right? So if that's what they look like in a vision transformer, this is what it looks like in a multi-layer perceptron. So this is a visualization of a transformer, specifically a nano-GPT. I love to show this just because I think it's one of the best visualizations out there. But what I want to show you guys here is at the very beginning, right? So at the very beginning, you have your context. So imagine this as the context length, right? I think there's, however many this is, 8, 1, 2, 3, 4, 5, 6, 7, 8, 10, 12, something like that. But this is the max context length. And this is a simplified example where there's a vocabulary of only three, right? So here, if I hover over this, you see how it says, nVocab underneath there. So nVocab is basically how many different words you have in your vocabulary or dictionary. So if you've watched the Carpathy tokenization video, right? You know that modern tokenizers probably have somewhere like 30,000 or even 100,000, or I think Google now uses almost 200,000 different words. And each of those words is basically an embedding. So here, this is a simplified example, right? So there's only three possible types of words or tokens in the dictionary. And you can have a token A, B, or C. So you can see here how there's three possible embeddings. The embedding for each one is basically one of these columns. So you have three, one for A, one for B, and one for C, right? And those are your token embeddings for the different words in your vocabulary, right? And each one of those columns, right? So here, I don't know what this is, maybe 20 something, 100 something, different numbers that represent basically one embedding for one letter. So A has one of these, B has one of these, C has one of these. What these vectors are is a high-dimensional vector that represents kind of the semantic meaning of that word. So I have here a two-dimensional representation of what a word embedding might mean, or a token embedding. And one way to think about it is that these token embeddings are kind of learned, right? So we feed a huge amount of data to them. And then over time, you basically get these vectors. And here, there are two-dimensional vectors so that we can visualize it with our little monkey mind easily, right? But you have to realize that it's not actually a two-dimensional vector, it's a, whatever this is, C-dimensional vector, right? So it's existing in a much higher dimensional space, but the analogy of this is kind of what you want to be thinking of, right? So each token in the vocabulary has some kind of meaning that is represented by a vector. And the kind of positions between the angles between those vectors has semantic meaning, right? So you can see that the vector that represents the word king and the vector represents the word queen, the kind of angle between those is similar to the angle between man and woman, right? Because like semantically, there's this concept relationship there that is represented in this vector space. Here it's just two-dimensional, but there's the high-dimensional version of that, right? So that's token embeddings. So position embeddings is basically now the same idea, except rather than doing it for the three different possible letters of which here there's only three possible tokens, A, B, and C, you're now doing it for the positions. So you see how the total number of possible positions here, which is limited, that's fixed. Your transformer has a fixed context. Each of those positions is represented by one position embedding, right? So you see each one of these columns here represents the embedding for each one of these positions. So the position zero has a specific vector, the position two has a specific vector, the position three has a specific vector, and so on, right? And when you want to pass that information to the next level of the transformer, you basically combine these. In this example, they're just literally adding them, so you can see they're just adding the position vector for here. The first letter is C, so it's adding the token embedding for C, and it's adding it to the position embedding for the first one. If you go to the second one, it's adding the position embedding for B, and the position embedding for the second position, and then the token embedding for the letter B, right? But it can get much more complicated, right? And maybe what did these vectors look like, right? So I have a picture of that, too, just to kind of show you guys what that looks like, and here we go. This is the position embeddings for 528 tokens. So here you have basically zero to 500 corresponds to this here, right? So you have zero to whatever this is 12, because there's only 12 possible, and then there's the length here, right? And that's basically showing you here. So this feels a little bit more hand engineered, right? And it's true, right? The vectors for the tokens, these are learned, and they have this kind of semantic concept nature to them, right? But these position embeddings here, they're just these kind of weird, hard-coded, kind of looking repeating patterns, right? And how do we even get to this? Like, why are we doing this? Why are we have these weird, repeating patterns that we're using to represent the positions at each of the points in our context? Okay. It's a little confusing, but, and this whole stream is going to be a little intense, but, you know, maybe if you stick around, you'll kind of understand a little bit more. So let's go back to our paper here. So that's what the PE is, position embeddings. The row PE, so the next two letters here, ROPE, comes from this paper. So basically, this is originally a 2021 paper, but they've updated this over time. Now it has 2023, but this first came out in 2021, and this paper was called row former. So, row rotation, rotary, and then former for transformer. So ROPE embeddings, row position embeddings, come from this paper here. Rotary position embedding is like the full name of that, right? This is a paper by Joy Yee Technology Co, in Shenzhen, which is kind of like the Bay Area of China, basically. So we're all the cool tech stuff happens, but in this paper, they basically say, okay, we have these position encodings back in 2021, where they released this, it was still relatively primitive types of position encoding. So they say, we want to be able to basically model the dependency between the elements at different positions in the sequence, right? So they want to have to encode the position of the sequence in these embeddings, right? They want to come up with a set of these little vectors here that represent each of this position that will allow you to basically extract meaning from the relative position of those tokens in the sentence, right? Because ultimately, what you're going to be doing with this, with these embeddings here, these vectors, is you're going to be putting them through a self-attention mechanism, and the self-attention mechanism is going to be doing some dot product, right? So here's the QK and V, here's your dot product down here, so this is like your actual dot product, right? Sometimes also called the attention map. And what is a dot product, right? Well, if this is vectors that represent some semantic meaning, the dot product is when you take two vectors like this, right here, you have a vector x and a vector y, here they're dividing it by the magnitude, so you have normalized vector x and a normalized vector y, the angle between those with this little brackets here, you can also think of it basically like the the cosine of the angle between them. So the more those those vectors are kind of agreeing, the smaller that angle is between them, and that is the geometric definition of a dot product. So the geometric definition of a dot product is that. But the more mathematical definition of a dot product is this, right? So here you're basically saying you have a vector a vector B, that's the same as the magnitude of a times the magnitude of B times the cosine of the angle between them. Or in the way that it's shown here, you basically divide by that magnitude and then you get a dot B divided by the magnitude of a and B is the cosine of the angle between them. But the full kind of like element by element definition of it. So here you have the same a and B, but rather than having them in vector notation like this, you basically each element where each of them has an elements, basically what the dot product is, is you're just multiplying first element of a times the first element of B. Plus second element of eight times the second element of B plus third element of eight and so on all the way to and where and is the length of the vectors. Okay. Let me know if you guys want me to pause at any point, right? Because I realize I'm throwing a lot of information at you guys and this stuff is pretty dense. So happy to kind of pause and explain some of this stuff. Okay. We investigate various methods to integrate positional information. We propose a novel method named rotary position embeddings or rope to leverage the positional information. The rope encodes the absolute position within a rotation matrix. And meanwhile, incorporates the relative position dependency in the self attention formulation. The key things of rope. The key properties is that it's flexible with respect to the sequence length. It decays inter token dependency with increasing relative distances. So that second one there. Decaying inter token dependency with relative or increasing relative distances. What that means is that as you get further and further between it, right? So here if you're each of these position embeddings represents the position embedding for each of these positions in the token sequence, right? The dot product between this this one and this one should give you less dependency than the dot product between this one and this one. Right? So the closer you are the closer one token is to another token in this sequence, the more similarity there should be in the vectors between that represent those positions in the sequence. Okay. So that's kind of one of the key things that you want in a position embedding the capability of equipping the linear self attention with relative position and coding and the flexibility of the sequence like the flexibility of the sequence length comes from the fact that you can you can kind of expand this so it doesn't really necessarily matter. You could you could actually wait let me let me get to that later because I think in order to do that, I'm going to have to introduce something else. Okay. So anyways, that's the row former, which is where the original rope comes from. So in the original rope paper, the way that they basically get to this right in rope position embeddings are hand designed it's someone came up with this idea. Right, like someone imagined this they thought about it and they were like, okay, here's what we're going to do here's here's what a rope rotary position embedding is. And you can go to this paper and follow their intuition and their intuition is based around this idea of basically you can keep scrolling you see more math, more math, look at this and then rotary position embeddings is based around this idea of a rotation matrix. So basically the way that the original people who came up with this, Intuited and even thought about doing this is they were thinking about rotation matrices right. So if you actually go to figure one here, which is in the row former paper figure one right. This is the way that they're thinking about it. They're thinking about, okay, you have these vectors right that represent here you have different tokens in a sequence. So enhance transformer with rotary position embedding right it's tokenized by word usually that's not the case anymore usually your tokenization is cutting up words into little chunks. But for now imagine that your tokenization is just cutting your sentence into words and then each of these words represents a token and that token has some dimension D right it's some vector in some high dimensional space. But here they're saying okay pretend that D equals to just so it's two dimensional and we can understand and see it and visualize it and they're saying okay what a rotary position is really doing is it's taking your vector that represents the concept of the word enhanced and it's rotating it by some angle right some some there's some angle that's rotating that vector. So you see here how the vector which is originally x 1 x 2 this vector with this dotted green vector here is now becoming this vector here right. And then that is now the position encoded query and key right so what we've done is we've basically said okay. If the dot product. Let me go back to here if the dot product tells me dot product between two vectors tells me the similarity between these vectors right then the dot product of word embeddings or token embeddings should tell me the similarity of words from a semantic viewpoint. But I also want to have some notion of positioning in there as well right so if the word king and queen are right next to each other that should be giving me a different attention score then if the word king and queen are separated by three paragraphs right so I want to modify the road or in the way that these guys think about it I want to rotate those vectors to basically affect the dot product so. What they're doing is they're modifying these position embedding vectors here and they're thinking about it as a rotation matrix in order to rotate the semantic meaning vectors and end up with these properties that they wanted at the beginning here that if you have decaying inter token dependency with increasing relative distances. Okay it's a pretty hairy concept and this is just not very easy to follow either right this type of rotation matrix exactly what the fuck is going on so I guess maybe here's one extra thing here the inner product will decay when the relative position increases this property coincides with the intuition that a pair of tokens with the long relative distance should have less connection. Okay with that said I'm not actually going to go through this mathematical derivation of it I'm going to go through this one instead so this is a paper that came out later right so this is a paper from 2023 this is a paper by meta platforms or Facebook and basically they take the rope position embeddings from this 2021 Sue paper this one right here the one that I just showed you and they say okay working to do this stuff called position interpolation the position interpolation isn't. It's not necessarily super important because they end up coming up with a better version of it in this long rope paper which is the one that we actually want to do but this paper here I want to go to it because they have a very nice concise mathematical definition of it right so this is in the background section of this paper they basically say here's a much more concise and compressed mathematical definition of what rotary position embeddings are and I think that this definition here is probably a little bit easier to explain and understand that. So I think that this is a very important question and these like four pages of more of different math that they do in the original rope paper where they basically come through this analogy of the rotation matrix rather than here in this Facebook paper they just kind of get directly to the end of it right okay so let's read this section here transformer models require explicit positional information to be injected right so transformer models are just sequence to sequence they don't know what order these patches are coming in and I'm going to go through this. What order these patches are coming in right because ultimately they're going to compute an attention map which is just going to multiply everything by everything right you can have the top product of all of these with respect to all of these so. Technically there's no position information in there which means you have to add it explicitly right so that's kind of we already knew that that's why we have. Position embeddings because transformer models they need the explicit position information. Typically in the form of positional encodings to represent the order inputs we consider rotary position embeddings which is the position encoding used in the llama model so the rope is what the llama models use given a position index m from zero to see so the position index m is basically going to be where along this sequence length are you right so position index zero would correspond to here position index one correspond to here two three four five six seven eight nine ten. So I guess this has a context length of ten and for each of those ten you're going to have one position embedding right so you see others ten of these corresponding to the position embedding for each of these ten right okay. The embedding vector x from x zero all the way to xd so you see this is the ten different embedding position vectors here right so you have your ten position vectors and you have your ten. So you can see that the embedding sequence sequence positions index d is the dimension of the attention head so the d dimension you can kind of think of it as kind of the length here right so there's some dimension to this vector here you can think of it like the dimensionality of the space in which this vector lives right so in this example this is a two dimensional but here in this one when they show you this they also use d equals two because they want to show you in 2d because they want you to build that intuition of rotating a vector in 2d but really when we're dealing with real world things the d is much higher right so these are vectors that are living in a much higher dimensional space it's just that for all of our figures and examples we usually use d equals to because it's just easy to see as a human. Okay rope defines a vector valued complex function f x m as follows okay so 

this is a function which consumes an embedding vector x and outputs some other embedding vector x you can see here that this f of x consumes x and then it also outputs some vector and you can see how this vector has the same amount of terms here right you see this blah blah blah blah blah blah blah blah blah blah blah blah all the way to blah blah blah blah blah blah blah d over 2 minus 1 right so this function x basically just consumes vectors and outputs of vector so okay where I use the imaginary unit and theta j is 10,000 times negative 2 j over D okay using rope the self attention score is this so this get's a little hairy here and but before I move I want to show you guys this here this dot product This is the dot product between the Q and the K. The Q and K are the query and key vectors. So really all that's happening here is you're doing a dot product between your Q and your K, If you go into your self-attention, you're just doing these dot products and notice how here you're basically multiplying the element of every single thing, adding them all together and then boom, that's your attention score, right? You're doing that for the Ks and the Vs, right? So it's basically the mathematical definition of the dot product that we had here, right? So all that we're doing in this here is you could think of the rope embeddings as basically just consuming the original vectors, modifying them a little bit with some positional information, and then you're doing the dot product. Okay, but what is this RE? So this RE stands for real axis, as opposed to imaginary axis. And in order to understand what that even means, we're gonna go to what is known as Euler's identity. 

Okay, so Euler's identity is often called one of the most beautiful pieces of math in the world, and basically this is the equation. It's e to the i pi plus one equals zero, or differently e to the i pi equals negative one. And the way to think about what is going on here is that when you introduce imaginary numbers, you can basically use this formula here and think about a real axis here. You see this RE, the real numbers, and then I am the imaginary numbers, right? So if you do that and you use this E, and we know that you can basically break up E with a cosine and a sine, right? So you see how cosine this is real, and then plus i sine, that's an imaginary. That means that when you're moving around with E to the i pi, I think, psi here, what you're really doing is kind of tracing a circle that goes into the imaginary plane out of the imaginary plane, into the real, out of the real, into the real. So you're kind of tracing these circles. And I'm kind of blasting through this, so it's kind of hard to exactly understand what's going on. But the important kind of thing to realize here is that whenever you see these E to the i, something, E to the i, something, E to the i, something, those are just signs and cosines, right? Like, I have a couple other gifts here just to kind of motivate this, but here's the same thing in a more visual form. Here you have x plus i, y, right? And you can see how that cosine of something, sine of something, you see how you're kind of tracing out this kind of corkscrew in the x and y. Here it's x and y, but imagine y is the imaginary, and then x is the real. Here's another kind of gift to show that, right? Again, here it's y and x, but you can imagine x being the real axis and then y being the imaginary axis, right? So here, going back to our paper when they say, rE, what they're basically saying is that, okay, we put imaginary numbers in here, which means that our answer is going to have some component in the imaginary axis and some component in the real axis, but we only are interested in the real axis version of that. Okay. I'm going to pause there for a second. Why are we engineering these positional embeddings? Shouldn't we build a model to learn them? Isn't that the point of deep learning? I'm going to get to that eventually because I do have a commentary that's actually very similar to what you're saying. 

I know exactly what you're thinking and I came to the same conclusion, but I think if we jump to that right now, it's going to be a little bit too confusing. Let me finish with what's going on here. Okay, so you have your rope position embeddings, which is basically rotating your vectors, right? So you're embedding vector x here from zero to D minus one. That's these things here. Let's go back to where we were. And rather than what you're doing here, so in this simplified example, you're literally just adding the token embeddings to the position embeddings, the rope embeddings are a function that is rotating the semantic embedding, right? So you're rotating it. So it's a function that is applied to the embedding. So your rope is just rotating these queries, rotating these key vectors, which are some high dimensional vectors. And because of Euler's identity, right in this imaginary real axis, we can basically replace these E to the i, whatever hell over here, we can replace that with cosines and signs. So you can see here now we've basically gone from this equation here, where we have this E to the i, theta j, and we've now replaced it with a cosine and a sine. And the important thing to see here is that there's this M minus N, right? So basically that M minus N is only ultimately makes this entire thing, this entire self-attention score, only dependent on the relative position M minus N. Okay, and that's really what we were after, right? Is we want some kind of way of making the tokens that are close to each other have more connection, rather than tokens that are far away, right? And our rotary position embeddings is a way of basically using signs and cosines, these kind of repeating functions to create this mess, which I showed you here, right? This is just a bunch of signs, cosines, with different frequencies, which are coming from that theta j, right? So this term here, this theta j equals 10,000 negative two j over d, like that term right there, right? You can see how here it's theta zero, then theta one, then theta all the way to d. So basically theta zero would correspond to this, the first dimension of this, theta two would correspond to the second, third, fourth, fifth. So you can think of basically as we move down each of the components of this position embedding, we're changing that theta, which is you can kind of interpret it as the frequency, the frequency kind of determines how fast these signs and cosines kind of go up and down, right? Like how fast they oscillate. So something that has a high frequency is gonna go up and down much more than something that has a low frequency. So ultimately what a position embedding is is basically these hand designed sign and cosine functions that are rotating these semantic vectors in a way where basically the lower dimensions and higher dimensions have different frequencies. And all of that is hand designed to ultimately come with those three properties that we had here of decaying inter token dependency, capability of equipping linear attention with relative position coding and the flexibility of sequence length. Okay, so what does it mean by flexibility of sequence length? What that means is we're gonna cut to this figure here. 

Actually, I think there's a version of this figure in this paper that is a little bit more clean so it's a little bit easier to understand. Okay, so your rope, right? Your rope position embeddings, it's a continuous function, right? So if we actually look at the equation for this, right? There's no, you can have a very, very high D or a very, very low D, right? So your J and your D is kind of your choosing what that discretization is, right? So here, you're choosing to discretize it at these points here. So basically you're taking this function which you can evaluate at infinite precision and you're saying, okay, but I only have 2048 positions along my context. So therefore I just need to plug in the 2048 different positions and I'll get the position embedding for each of those, right? I'm basically taking this continuous function of signs and cosines and then discretizing it into these little dots, right? So each of these, each of these vectors here that represent each of the discrete positions in the sequence are gonna be represented by one of these points and it's gonna be basically some vector that is the position information at that sequence index. 
Okay, it's a bit like analog to digital conversion. Yeah, it's one way to think about it, right? And it's kind of like, but the more important thing to realize is that this is all arbitrary, right? Is that all of these things are engineered, right? It's not, there isn't some magical mathematical reason why this is the most optimal position embedding, right? It's not. This is some intuition that comes from some guy at whatever company this is, Ju Yi Technology Co, who was like, wait a second, what if we just rotate these vectors and we basically choose these rotation angles based on these signs and cosines, and then we define a function that basically tells you the rotation angle for specific things at specific that is dependent on the position in this sequence. And then we just sample, however many we need based on the context length, right? So if you have a context length of 2048, you just sample this function for d equals zero, d equals one, d equals two, like all the way to, or it's not d, the dimension would be the height of this, and then the individual, I token ID, so like then this, this J here, 

theta D where, or just this J right here, see? 2J, and then J is actually the number that you change based on the sequence length, or where within that sequence you are. Okay, but again, all of these are hand engineered, they're just hand designed, right? So in this meta paper, they took the rope position embeddings which came out of this 2021 paper and then came up with this position interpolation idea. And now long rope, finally we're getting to the paper that we're reading today, long rope is a remix of rope, which is a, or long rope is a remix of the position interpolated rope, which is a remix of rope, which is a remix of position encodings. So we're on a remix of a remix of a remix, and this particular remix came out this week, 21 February, 2024, it's by Microsoft Research, and it's a variant of rope called long rope. Okay, so now that we've gone deep into position embeddings and what the fuck is going on there, let's take a step back and let's look at this paper and what exactly they're saying. Okay, so large context windows is a desirable feature, due to high fine tuning, scarcity of long tasks, and catastrophic values introduced by new token positions, current extended context windows are limited to around 128 K tokens. This paper introduces long rope that for the first time extends the wind of a pre-trained LLM to an impressive 2048 tokens. Okay, so that kind of sounds like what Gemini did, right? Gemini got really good long context, so this seems to be right around that same magnitude, right? With up to 1K fine tuning, blah, blah, blah, blah. This is achieved by three key innovations. We identify and exploit two forms of non-uniformities and positional interpolation through an efficient search. This paper's pretty gnarly as well, so I'm gonna go into these things once we get to them. They also introduce a progressive extension strategy where you first fine tune a smaller context length and then you gradually increase the context length, so you're training with a small context, and then you fine tune with slightly longer context, and then you fine tune with even longer context, and then you fine tune with even longer context, that's a progressive extension strategy. You're progressively extending the context length with a series of consecutive fine tunes. 

They do these experiments on LLM2 and Mistral, which is kind of weird. You got Microsoft Research doing research on LLM2 and Mistrel. This is why OpenAI needs to release their own open source model or their own open model, so that at least the Microsoft researchers can use the open Microsoft model, but there is no open Microsoft model, so they have to publish their results on LLM2 and Mistrel, which are like competitor companies. It's kind of weird situation that they've gotten themselves into. But the key part about this long rope is that you don't need to basically change everything. You can retain the original model architecture and just do this progressive extension strategy with a series of fine tunes and end up with a version of your model that now has a significantly longer context. So here, figure one, figure one in a paper, especially if it's on the front page, it's gonna be the most impressive figure. So what are we looking at here? We're looking at an X, Y. Plot, the Y axis is perplexity. Perplexity is kind of like a measure of goodness, like how certain is a model when it outputs tokens, so a low perplexity is good and a high perplexity is bad. So here we have the bottom, we have context window size. So this is the length of the context window. You can think of it like the length of this here. So here it's just 10, but here you have everything from 8,000 tokens to 2048 tokens, which is close to the Gemini 1.5 level. And you can see here that for a bunch of other models, such as code, llama, yarn, llama, long rope, llama, you know, they're good at a small context, but if you try to give them a large context, they kind of just explode, right? So you see how the perplexity explodes, perplexity explodes, perplexity explodes. So pretty much all these models, once you get the context past a certain size, it's just gonna explode, and it's just gonna be a bunch of nonsense. But you see these two lines here, this dark blue line, and then this slightly less dark blue line, these are long rope, long rope mistral, and then long rope llama. So these guys took llama two and mistral, they did this progressive extension strategy for longer contexts with their long rope position in beddings, and you can see here now that the perplexity is still pretty much just as low as it was at the beginning, but with the 2048 context. So what they're saying is look at this, we managed to extend the context of these two LLMs that aren't even our LLMs, right? This is Meta's LLMs and then mistral, is it mistral, is the name of the company? Mistral's LLMs, and look at that. The perplexity just gets a little bit worse, right? So it loses a little bit of performance when you have a 2048 context length, but that level of perplexity for that size of a context is still pretty good. Okay. A pre-trained LLMs context window can be extended to around 128K by fine tuning on longer text. This is kind of the more naive type of extension that they compare against and that they say is basically shit compared to their strategy. When extending to extremely long context windows, the attention becomes dispersed as it's spread thinly across numerous token positions to graining performance on the original context. Okay, that's a little bit of a theoretical analogy there, but again, it's very difficult to really make these types of analogies because we don't really know what the fuck is going on here, right? It's like really, you know, like, spread thinly across numerous token positions. Like, what does that mean? What does it mean that this is spread thinly, right? Like, there's no, that's not like a formal mathematical statement. That's just like this random guy's opinion about what's going on underneath the hood. Okay. One approach to mitigate the first challenges to interpolate the rope positional embeddings. Rope positional embeddings, you have a couple pieces of work that have gone on after that. You have the position interpolation, which is that paper that we were looking at. This one here, this meta paper here where they do the interpolation interpolation. Then you have NTK and you have Yarn, which are other remixes of rope, but we're not going to go into position interpolation and TK and Yarn because basically they just don't really work as well as this approach, right? So better off just learning the approach that works rather than learning the kind of three other approaches that failed historically. So they basically say that positional embeddings exhibit complex non-uniform information entropy. Such subtle non-uniformity is not effectively leveraged by leading existing approaches. So they're going to try to figure out these types of non-uniformities. Specifically, they're going to be considering varying rope dimensions and token positions. That's not going to mean anything until we go a little bit into the details so we're going to kind of gloss over that. Better initialization for fine tuning, which is important because you're going to progressively extend the context length over a series of fine tunings. It identifies effective, so long-rope, identifies effective rescale factors for ropes rotation angles for each rope dimension based on the token positions. Okay, so ropes rotation angles for each rope dimension based on token position. Okay, so the token position is this, right? It's like where along the sequence are you, right? The dimension is this part, so here you have the lower dimensions of the position embedding, here you have the higher dimensions of the position embedding, right? We know from our intuition of the signs and cosines that the higher position, higher dimensional position parts of the position embedding are going to be high frequency. The lower dimensional parts are going to be low frequency, so you can think of like a sign and cosine something that's bouncing a lot and then something that's bouncing much slower, right? So they're going to be scaling this, right? So they're basically going to identify rescale factors based on the position. So they're going to have position dependent rescale factors and what those rescale factors are doing is they're changing these rotation angles, right? So if the intuition around what rotary position embeddings is, is that you're basically rotating these vectors by something here, right? Where that thing there, that theta, that's changing the basically the rotation is dependent on this i, where the i is the position in the token sequence, you're basically going to be multiplying that by some factor, some rescaling factor, aka tweaking the position embeddings such that you get better position embeddings that work, right? And the crazy thing that they're going to do in this paper is that they're not going to just like guess, right? So in these other papers, these are all just human guesses, right? So some guy came up with this, some guy came up with this, some guy came up with this, right? Like where does this number 10,000 come from? It's just some random dude came up with this 10,000, right? But in here, what they're going to do is rather than kind of guess at these rescale factors, they're going to search for the rescale factor. So they're going to do an evolutionary search algorithm to basically find the optimal values for these rescale factors that are dependent on the token positions, which is the level of complexity of this is fucking insane. And that's going to eventually get to a more complicated point. Okay. You guys are popping off. Is position embedding something similar to what they did in the Nerf paper? Not really Nerfs don't have any position embeddings, right? Nerfs are a neural network that basically implicitly define a field where a field is something where you can query. You can query and say, what is the opacity at this point in space? What's the opacity at this point in space? That's what a Nerf is, right? It basically just tells you what the color of a something is at some point in space. This is more how do I tell a transformer information about where I am in this sequence, right? So position embeddings are just about how do I make up some vector that basically gives my transformer the information it needs to know that four and five are closer together than three and five and that four and five are closer together than seven and nine. Right? So you want to basically know the relative position of things within the sequence. Okay. So we're going to dive back in the math. It's going to get pretty hairy, but hopefully we come out of this with a little bit more intuition. So transformer models require explicit positional information, often in the form of position embeddings to represent the order of the input tokens. Okay. For a token at position index n, right, where the position index is where you are in the token. So here this token is in position index four. This one's in three. This one's in two and so on. It's corresponding rope encoding can be simplified as follows. So here's the rope encoding for that position. Okay. So you see how this is a vector. It's something, comma, something, comma, something, comma, and each of those something, comma, something is basically this. So it's something, comma, something, comma, something, comma, something. So that rope vector is basically just one list of numbers for an individual position. Right. So that's it. That's it's one list of numbers for an individual position in the context or the sequence index. So you can see how the value for each of those positions is dependent on this theta where the theta is dependent on i. So the very first one here is going to be theta zero. When you plug in i equals zero, you're going to get a very low frequency into this, which means that here at the beginning of the vector, you're going to have low frequencies. Right. And then here at the bottom of the vector, you're going to have high frequencies. And the high versus low frequencies comes from the fact that the theta, which is increasing, you see here theta zero, theta zero, theta one, all the way to theta d over d minus or two minus one, where this is. Basically d is the dimension of that. Is the rotation frequency. Right. Rotation frequency because this. Right. So this in your head, think about a corkscrew in a very high dimensional space. Okay. Context window extension ratio s and the positional interpolation. We define s as the ratio of the extended context length l prime to the original length l. Okay. So you have your original context length l. So in the case of a llama to right, llama to was originally trained with 4096 tokens. Okay. 4096 tokens is going to be your original context window. Right. L. And then you're now going to do an extended context length, the extended context length is eventually going to be 2048. But you're going to get there slowly, right. So you're not going to go directly from 40, 4096 to 2048. You're going to basically progressively extend it. Right. And every time you extend it, you're going to have this ratio of the original context length to the extended length that you want. Okay. So if we have this extended length, that gives us this extension ratio s, which is how much how much you're increasing the context every time you're doing this fine tuning. And you can modify the position embedding. So here's your position embedding. You're now modifying that with this extension ratio s. You can introduce this rescale factor lambda, which makes it a little bit extra confusing. But largely this right here is basically this right here. So it's the same vector. It's the same vector here, the same column right here, except now you're kind of basically multiplying it. You can think of it rescaling it such that it represents, rather than using position one, two, three, all the way to 2048. So I think I got that flipped. Okay. I'm like concerned that I'm losing people here. Okay. So linear position interpolation. This is from the meta paper. They basically set to lambda equals s across all rope dimensions. So they just extend it or interpolate. However, this makes the position information very crowded hindering the models ability to distinguish closely position tokens. Therefore, PI tends to underperform at high extension ratios. Okay. So what they're saying is that you have some function, right, that defines your position embedding at every single point in the sequence. One way you could do it is you could say, okay, well, now I don't just have 4096 tokens. Now I have twice as many tokens. So what I'm going to do is I'm just going to extrapolate. I'm just going to take that function, run it for another 2048 tokens, and then just basically these are the individual values of my position embedding here dot dot dot dot dot dot dot dot dot. That's what I'm going to have. So each of those dots basically represents this, right? So in the extension, what I'm doing is I'm basically just taking this and then just adding a second version of it right next to it where I just kind of naively just evaluate at the same one, right? That's naive extension or extrapolation. In position interpolation, what I'm going to do is instead I'm going to basically interpolate between them. So rather than basically sampling this function at higher levels of D rather than basically saying, okay, rather than going all the way to theta D over 2 minus 1 where i or theta i rather than going to 2048. Now we're going to go to 4096. Rather than doing that, I'm going to basically sample twice as much. So I'm going to interpolate. So now the total number of dots in this one here is the same as the total number of dots here, but now rather than extending this function, I'm basically sampling it twice as frequently. But the problem with position interpolation is as they said here, you're kind of the position information becomes very crowded. So another way to think about what's happening there is that if we go back to this intuition of position embeddings specifically rotary position embeddings as rotating these vectors, right, whenever we were doing this, right, we were rotating the vectors by this amount, by this amount, by this amount, by this amount. So there was more rotation happening between subsequent sequence positions in your context. If you position interpolate now, it's almost like you're rotating by smaller amounts. So you're only very tiny rotations. And if you're just using very tiny little rotations, then you know, the difference between subsequent tokens and tokens that are a little bit further apart, isn't that much anymore, right. So then you're starting to lose some of that positional information that is the whole point of these positional embeddings. Right, so that's kind of the the fact that they become crowded as they're saying here. Okay, so then you had an NTK based interpolation and TK based interpolation decides to split up the low frequency lower or high frequency dimensions and higher or low frequency dimensions. So basically they say, okay, we're going to rescale based on where you are in this sequence. Right, so if naively extending is taking this and then just evaluating it more. If interpolating is taking this and then basically just sampling it higher, so you have a longer one, then what yarn does or yarn and NTK is are basically even more kind of complicated, heuristically, heuristically designed things. In the case of NTK, you kind of like your rescaling depends on where you are in the sequence. And in the case of yarn, it's even more kind of hodgepodge. They divide the rope dimensions into three frequency based groups, each with a different interpolation strategy. So basically they're going to say, okay, we're going to interpolate this first part here differently than we interpolate this part here differently than we interpolate this part here. Right, so now we've said, okay, there seems to be the frequency here at these ones here. At these ones here is different than the frequency down here. So let's interpolate the frequency here differently than we interpolate the frequency here. Right, so I feel like yarn is like the, the, the, the, basically the limit of complexity, maybe not because this paper is basically even more complex than yarn. But you're taking something that's already very hand designed, such as position embeddings, further hand designing it with rotary position embeddings, further hand designing it with basically splitting it up into these frequency bends and then interpolating differently depending on which frequency being you're in. The key of yarn lies in its grouping of rope dimensions, which currently depends on human lead empirical experiments. This may result in suboptimal performance for new elements. Right, so basically they're saying, okay, this is some random human design this we like they're just kind of doing a couple experiments and saying, okay, this one seems to work the best. And that's how we got yarn, which is according to this paper, the current state of the art, although I'm sure there's better versions that are secretly hidden in Google. And you can see here. This actually is kind of like yarn, but you see here for dimensions of 40 to 64 versus dimensions 60 to 64. So the 60 to 64 those dimensions represented by this green line that represents the dimensions down here, the ones that are higher frequency. Right, they're going to be interpolated differently. So that's this green line, this green higher frequency line. You're interpolating that differently than you are the dimensions 40 to 64, which are the lower. Right, so you're basically grouping and interpolating differently depending on where you are in this position embedding vector, where are you here at dimension zero or are you here at dimension 20 or 128 or whatever it is. Okay, whack as fuck or latest fuck, okay. All right, where are we at? We were here, so we were looking at. So taking your rope and coding, now we have to rescale it because we want to increase the context length. Okay, how are we going to rescale it? Okay, well, we could interpolate. Okay, we can interpolate, but then we can intelligently interpolate using based on the frequency, which is what yarn does. But, okay, how do we make that even better? So inspired by NTK and yarn, we noticed their gains from nonlinearity, specifically in considering different frequencies, a crop, rope dimensions for specialized interpolation. So what do they mean by nonlinearity there? So what they mean by nonlinearity is that you're kind of arbitrarily picking three frequency based groups. Right, so it's not, it's not like you are have some kind of linear function or some kind of like smooth function that says. Okay, here's the how we're going to interpolate from here to here, it's like you're basically coming up with these discrete boundary points, we were saying, okay, all the dimensions above this line, we're going to interpolate this way and all the dimensions below this line, we're going to interpolate that way, right, so you're coming up with these arbitrary nonlinear basically boundaries, we were saying interpolation is going to occur this way here and this way here. Right, and that's what they're saying here, right, they're saying, okay, well, if you can just kind of make these nonlinear boundaries and say we're going to consider different frequencies and interpolate differently for different frequencies. Can we just not hand design those? How so basically, however, current nonlinearity rely heavily on human designed rules. This naturally raises two questions is the current position interpolation optimal and are there non or unexplored nonlinearities and that's where they're going to be doing the evolution search. So their answer to this is they're going to use evolution search to discover better non uniform positional interpolations guided by perplexity. Okay, so what this paper is, what long rope is, is it's basically yarn this paper here, but rather than kind of arbitrarily picking these frequency groups based on some human design, some human just picking it, they're going to instead try different values of it and then try this value and then what's the perplexity try this value, what's the perplexity try this value, what's the perplexity and then eventually the ones that that that's a form of search, right, but the search is an evolutionary search because you're going to basically have the evolutionary components to that which are taking the ones that succeed, the ones that have high or the ones that have low perplexity, which means that they're useful, which means that you're finding good points, good values for these rescale factors. Those are the ones you're going to make variance of those and the ones that don't perform well, you're going to get rid of them, right, so you're going to have this pool of different rescale factors and you're going to constantly be kind of churning through this pool, searching through this pool where the good performing ones get amplified the bad performing ones get removed and over time you just run this evolutionary search and eventually the universe will give you the optimal, quote, unquote, optimal versions of or optimal values for these. So these are the values for these rescale factors. Okay, rope dimensions exhibit substantial non-uniformities which are not effectively handled by current position interpolation methods, this is just saying again, okay, we have these non-uniformities non-linear kind of random arbitrary places where we're deciding to interpolate differently. Our searched solution shows significant improvements throughout the search, the rescale factors become non uniform different from the scale, these non uniform factor significantly improved llama to language modeling performance for 8k and 16k context, when does without fine tuning. Okay, the initial tokens and the input sequence should be extrapolated with less interpolation, the initial end tokens and input sequences, we hypothesize that their rope should do less interpolation, this is because they are receiving large attention scores. If this wasn't already complicated enough, they're not actually going to interpolate for the initial and hat positions. So what this means is that whenever they're extending the context length, they want, they want to extend this, this is the context length, they want it to be twice as long or 10 times as long, right, which means that they're going to have to make this, they're going to have to have more position embeddings, they're going to have to basically take all these position embeddings and either interpolate them so you have twice as many or extend them or do these more fancy types of interpolation based on where in the frequency you are, right. But another trick that they're going to do here is that for the initial and hat positions, they're going to do no interpolation. So when they're here and they're trying to figure out how to get more of these position embeddings so that they can have a longer sequence length here, longer context, they're not actually going to change the first couple. So basically the first three position embeddings here, they're going to use the same ones, right, and that's what they're showing here. So over the first end positions in the sequence, in the context, they're going to use the same position embeddings. And their argument here is that they're doing this because their ropes have large attention scores. I think the reasoning for this is kind of what we see time and time again. And I would call this the kind of attention sink situation, right, where basically in these transformer models, the first couple tokens end up being packed with information and end up being more important than the rest of the token. So it's like it's almost like the the transformers paying more attention to the beginning couple tokens just because of the way the inductive bias of the transformer itself, right, which doesn't really have a lot of inductive bias anyways. So my kind of intuition around this is that if you fuck with the position and coding of the first couple sit tokens or the first and hat tokens as they say here, I think it's 256 tokens is the first is the number for and hat, then it seems to just destroy the performance. So they're going to keep that the position embeddings for the first 256 tokens, the same, which is some weird ass hack. And that's actually one of the numbers that they're going to search for. So inside their evolutionary search, I think they actually try to find the optimal value for this and hat. Okay, finding three non uniform position interpolation effectively extends LLM context window in both fine tuning and non fine tuning settings longer extensions require fine tuning. So this is the idea of basically progressive extension. So you're going to take your pre train llama to you're going to then introduce the different position embeddings. You're going to those different position embeddings are going to allow you to have a longer context. You're going to fine tune on a data set that has these long contacts. And then in that way, you're going to basically fine tune the model such that it works with these longer context. And then once you have this size of context, then you fine tune that model on a slightly longer context. So this is you're progressively increasing the size of the context. Okay. And cutting to the chase significantly this long rope significantly outperforms PI and yarn both before and after fine tuning. So this is like the culmination of position embeddings rotary position embeddings position interpolation, which is an extension of rotary position embeddings and yarn, which is an extension of position interpolation. And now you have long rope. So it's kind of like that's the the historical kind of trace of where we get long rope and long rope is the best out of all of them. Okay. Problem formulation search space for ropes rescale factor. So as we were saying before, rather than hand designing or hand picking these, for example, the end hat, which is how many starting tokens do you keep without interpolation. And then the lamb to I, which is these rescale factors of like how are you rescaling or picking values for these position embeddings based on these sinusoides that are the original kind of rotation matrices of how much are you rotating these vectors. Right. Rather than hand picking those, which is what yarn did in this paper, they're going to be searching through them. So they're basically searching through it. So they're basically saying, okay, we don't know what the value for lamb to I is that's optimal. So we're going to say it's somewhere in between one and 0.01. And then we're going to basically just try we're going to use evolutionary search to try a bunch of different values of this lamb to I and we're going to use evolutionary search to try a bunch of these values for and hat. Right. Let's actually go to the evolutionary search just so you guys know what this looks like. So our search space and table four spans numerous position interpolation solutions. For example, blah blah blah, that's a huge amount of huge exploration space. Right. So if you guys have ever done hyper parameter sweeping the space that the more your search space that you have to search through is the longer that hyper parameter sweep is going to take. So any kind of search here you're searching over the optimal value for this rescale factor in the optimal value for this and hat. So that's a pretty large space. So they're going to search through it using evolution evolutionary search is a pretty simple you you already know what this is. You start with the population P of rescale factors that include the original values that were in the PI and TK and yarn papers. You randomly mutate these after generating the initial population you compute the LLM perplexity for each one. Then the top K individuals become parents, which means that you're going to kind of have them have sex, which means that the ones that perform well you basically create variants that are kind of like the parents and the ones that perform poorly you kill them off right they basically die from starvation and that's how evolution works the ones that work well reproduce and the ones that don't work well die off. But in this case, you're not using like survival in like an environment or biome as the which is what actual evolution is like evolution in the biological space. In this case, the evolution is can you get a low perplexity given the fact that you have this specific rope rescaling factor and this specific and hat value. Right so you're basically the long story short you're just trying to find the best value for that lambda and the best value for that and hat. Okay. Let's go back up here. Token position and this is kind of exact that we this is kind of what we talked about before right where you're basically you have your original context window size you want to extend it to a new context window size. And you're going to have this initial position and hat where basically you don't interpolate so here you can see the interpolation factor it at any token less than and hat is equal to one and then the interpolation factor at any token greater than and hat is one over lambda i. So basically you're saying that for the first and hat tokens I'm not going to change the position embeddings but then for the rest of those right any token that's greater than and minus one. Greater than and hat I'm going to be interpolating and I'm going to be interpolating using this rescale factor where the rescale factor lambda i is dependent on the token position but it's also based on this number here we don't actually that number is coming from the evolutionary search. So we found the optimal value of lambda i and that's what we're going to use. Okay so when they're doing this evolutionary search is important to kind of like what are they actually searching with right so the way that they're going to search is they're going to take a llama 27 be in a mistral seven B and they're going to basically fine tune it on red pajama which is like some small data set chunked into 128K segments. Beginning of sequence end of sequence tokens so you start with the beginning special token that says this is the beginning special token that says this is the end you're doing this progressive fine tuning you're trying to fine tune them on longer and longer segments of this red pajama and as you're fine tuning these once you fine tune them for whatever they say here 400 steps after fine tuning for 400 steps you evaluate all of them you look at the perplexity scores for all of them and of the perplexity score is shit you kill them of the perplexity score is shit you kill them. If the perplexity score is good you mate them and then you run another evolutionary search right and you kind of iteratively keep running these evolutionary searches until you end up with a magical value for n hat and a magical value for lambda that you as a human you have no idea why that's the best right you don't know why lambda hat or lambda is 0.3 is the best one you don't know why n hat 256 is the best one but evolution figured that out for you. Okay, progressive method achieves the target with just one k fine tuning steps. Okay, they're doing this on 8 a 100 GPUs and then 16 a 100 GPUs so this is definitely not casual you know this this is not like your mom and pop research shop this is Microsoft so they do have the GPUs to burn on this evolution. What else long sequence modeling main results we use two datasets demonstrate our generability proof pile in PG 19 so red pajama is the dataset that they used to do the fine tuning to find the optimal values of the rescale factors and the n hat. But in order to show you that their results are good so now they've actually found the best results are the best value for those now they're going to actually show you that it's good using proof pile and PG 19 so even with a context window that is 16 times longer a condition typically challenging their models long rope outperform state of the art baselines within 256 k context length and they have a. Plot here these charts are basically this so figure one is this top one here figure one is table five and then figure four is table six so what are we looking at this is the past key retrieval accuracy so past key retrieval is kind of like the needle in the haystack task that you saw in the Gemini paper but basically what they're doing is the model is asked to retrieve a random past key i.e. a five digit number hidden in a long document so. Basically they have a very specific little number that they're hiding in this massive context they're feeding that massive context into their llama to and mistral seven B that they fine tuned with this log rope position embeddings and then they're saying okay can it actually get the past key. And here's what they got you see so unlike perplexity perplexity lower is better but here in past key retrieval accuracy higher is better right so 100% past key retrieval accuracy means you got the past key every single time that we tested you for it and here you can see that the long rope mistral and long rope lava which are the two top lines pretty much always have 100% accuracy so. So even as you keep increasing the context length they're capable of finding that needle in the haystack versus these other ones such as yarn and even vanilla llama and mistral as soon as you kind of go past the context length at which they were trained they basically just are incapable of finding that. That passed key. We're like hack. Red pajama is one of the largest data sets with three trillion tokens okay good to know thanks man appreciate that. All right so they've shown that with their long rope position embeddings they can get good accuracy with a significantly longer context and the good accuracy here they're showing it via perplexity here they're showing it via a past key retrieval accuracy but another way to kind of see it is in this table here so comparison of long context l alms with the original llama to and mistral on hugging face oh. Open element benchmarks so here you have the original llama to model right which has a context window of 4000 and here they were saying okay well can we find the past key right but that's one thing right but another way to evaluate is like okay well does it still work on other benchmarks right like we increase the context length with this long rope shit and now it can find the needle in the haystack with a really long context but. Did we lose performance somewhere else and that's what they're trying to answer here so they're going to test on the original kind of benchmarks here you have mm l u hell a swag truthful q a these are kind of more. You have like math benchmarks this one is like slang benchmarks but basically what you what they're trying to test here is that okay is our long rope 2048 K now that we've fine-tuned it for a much longer context is it can be complete shit at math right have we lost performance and the answer is a little bit but but not significantly right so they show you here. The original llama gets a score of 46 on mm l u long rope gets a score of 39 that's not super great 78 on hell a swag 75 on hell a swag 39 on truthful q a 37 on truthful q a so it seems like you are losing a little bit of performance by just taking a pre trained llama to fine-tuning it for this longer context with long rope. You're able to now do these needle in the haystack tasks you know as they're saying here but you're not capable of you're losing a couple percentage points of your benchmarks right so it's getting it's getting a little stupider on these benchmarks but. It's kind of hard to argue right because you could also say that okay it's getting a little stupider but. There's probably benchmarks that you could create that require a context window of 2048 K right at which point the original llama to would literally be incapable of solving those benchmarks because it just doesn't even have the context to do so so maybe we need benchmarks that are better for long context right it seems like we're kind of in this weird place where we have benchmarks that are very short so these benchmarks you can evaluate you can test your model on them and that model can have a very short context because these are very short questions. And then we have this type of pass key retrieval accuracy where we can say hey can you find this very specific needle in this haystack but I think we need something that's a little bit better than both of those right I think we need basically a very a long version of mml you and hello swag and truthful where it's basically a very long context and the the task requires that very long context that's that's kind of what I would like to see I would like to see the kind of like long context version of these benchmarks here. Okay, and this is the same thing with mistral so mistral seven B has an 8k context window fine tune via long rope your mistral seven B long rope now has a 2048 K context window and you can see that even though you extended the context window and fine tune on those longer context it still does pretty good on your benchmarks which is what you want to see. 

Okay, I highlighted this part here in the related work section here they talk about retrieval based approaches so one of the current. Contentious points in in the machine learning world right now is whether or not rag will still exist I think that my opinion on this and I know this might be a spicy take is that rag is probably not going to be a thing in the future so So the original reason for rag was that these contexts of 4000 were just not enough to do some tasks right you wanted to be able to have more information that your LLM has access to. So the basic idea of rag or retrieval augmented generation or a retrieval based approach is that you're using an external memory aka basically a database where you're storing text information so if you need to answer something for a user. And you need more information you're going to go into your database you're going to find chunks of text and you're going to concatenate all those chunks of text into your context and then because now you have all the relevant information in your context you can answer your question appropriately and you get better results right. But as we have more and more and more and more context right we're extending our context. We're just going to be capable of putting everything in the context right so once the context length starts increasing to a certain point you probably don't need to be going to a database finding specific pieces of text that are relevant putting it into the context and so on right if you remember some of the rag papers that we read it was like. Okay the user is asking a question about food okay let's go to the database where we indexed all the questions all the other stuff that the user told us about food and we're going to find the three things that the user told us about food put those into the into the context so that when we answer this user's question about food we also have the relevant information or the relevant text tokens of previous times that they asked us about food right but the problem with that is that there's all this extra overhead right you're basically doing. You're you're embedding the question then using that to search a database and then adding that taking that all formation all that information sending it from wherever your database is sitting all the way to the GPU that's actually doing the inference loading it into the GPU VRAM and then answering your question right that's a lot of extra shit that you don't need to do if you just have the entire user like all of the information that the user has ever had in the context so. I think that going to databases and pulling information from databases that will definitely still exist and I think that using an embedding vector and cosine similarity to find things in databases I I think that still can exist but the idea of rag as a way to improve performance by selectively picking what you put in your context I think that's the part that's going to go away so I don't think vector database companies are fucked but. But. At they're not they're not going to be used for improving the performance because the context length is going to basically account for that they're they're going to be they're still going to be used but not for the stuff that they're necessarily used for now I hope that. Kind of made sense but maybe some of you are working at these vector database companies and you guys are like this guys a fuck him but I don't know that's just my opinion. 

Okay so what will we call the process of managing the information that is living in the context ideally you don't have to manage it right you just literally just put everything in there. 

Okay blah blah blah okay now we're at the conclusion here conclusion in this work we presented long rope a method that remarkably extends the context length of l alms to an unprecedented to 2048 tokens while maintaining their capabilities within the original shorter context window. We exploit two forms of non uniformities in rope position embeddings using an efficient evolutionary search so basically the non uniformities are coming from the yarn paper and the NTK paper and it's basically that. The first non uniformity is the fact that you pick the first and hat tokens and don't change the position embeddings for those right that's a non uniformity it's a non linear kind of just arbitrary boundary that you're deciding here where now suddenly we're not going to interpolate the position embeddings the other non uniformity is this. The yarn situation where you're basically bining the frequency or you're bining the dimensions of the position embedding based on frequencies and the bining is also arbitrary as you're kind of choosing these random rescale factors right dimensions 40 dimensions 60 wide dimension 40 wide dimension 60 right those are kind of arbitrary non uniformities so rather than kind of just picking those. Heuristically from our human intuition which is what they did in yarn what they're going to do in this paper is they searched right so they use this evolutionary search to basically find the optimal value for the rescale factor and the optimal value for the and hat so here's the actual search space right they found the optimal lamb to I and the optimal and hat with evolutionary search. Okay this offers twofold benefits it provides a good initialization for fine tuning and enables the eight times contact window extension without fine tuning so the good initialization for fine tuning is going to allow for progressive extension strategy where you're basically going to be able to change the position change the way you do these position and coatings and then fine tune your transformer on a longer sequence and then change the position again you do find the position and coatings again so you have a longer context and then fine tune again. But you can also just literally apply this without fine tuning and you get eight times context extension for free which is kind of cool. Building on this we propose progressive extension using 256 K length fine tuned LLM's to reach 2048 K context window size without extra fine tuning extensive experiments validate the effectiveness of long rope we envisioned that our long rope 2048 K models will enable many new long context applications and inspire further research and that's basically it guys. That's this paper it's the way I calling it is I'm calling it a remix on a remix on a remix where the original position embeddings got remixed into rotary position embeddings which got remixed into position interpolation yarn and NTK and then this long rope is basically a remix of all of those. And that's where we're at so now I want to go all the way back. So here's here's here's what I really think about this so way back a Khalil pointed made a very good point right Khalil said let me scroll back up why are we engineering these position embeddings shouldn't we build a model to learn them isn't that the point of deep learning and here's here's where I'm going to say Khalil is that you're right and I'm going to pull up the Moses tablets of deep learning this is the bitter lesson by rich son. We worship this document here in case you're not familiar but appreciate for my why saw my green. Yusa the looker full okay I think the looker full recommended a specific type of year Bama Te so that's the reference that he's a call out there but okay going back to this right so the bitter lesson basically says that general methods that left me. And basically says that general methods that leverage computation are ultimately the most effective right where we basically we are continuing to make the same kind of mistakes where we basically find hard coded things and human design things. And ultimately they just don't work as well as things that are learned right. AI researchers have often tried to build knowledge into their agents this always helps in the short term and is personally satisfying but in the long term it plateaus and even inhibits further progress a breakthrough progress eventually arise by an opposing approach based on scaling computation by search and learning. So what are the what is actually happening here and I'm going to go to an example from computer vision so in computer vision when you were doing convolutional neural networks right a convolutional neural network is basically using a convolutional filter or a kernel to basically go across the image like that right. So that's what you're doing in a convent way back in the day in the day of prehistoric computer vision we had things like gabber filters. So a gabber filter was designed by a guy called gabber right and he basically designed these he he was like okay we're going to design these little filters right and you see how this one is vertically oriented this one's horizontally oriented so that when you. parse through an image like this like you're doing right this one will trigger all the vertical edges and this one will trigger all the horizontal edges so for a very long time computer vision was literally the art of creating these weird little filters that would work well to in your final task whether it was classification so you were hand engineering these little filters to work on things eventually though we basically through that away and to this was before the bitter lesson but we eventually created confnets and we learned the filter so here's a confnet and showing you the basically you can think of it like that little gabber filter but at different levels of the connet and these aren't these aren't hard coded these weren't designed by a guy called gabber these were learned so basically we just push gradients into this convolutional neural net until eventually the convolutional neural net said hey this is actually what you want this is what you want you don't want these gabber filters this is what you want and it actually if you look at it kind of looks like gabber filters right so in a weird way the gabber dude kind of you know maybe he got he was validated because he his hand engineered kind of version of this ended up looking a lot like the ones that the computer ended up learning right so I think a similar kind of thing is going to happen here right where right now we're kind of at the point where if we look at these position embeddings right these positions and bettings, right? These position and bettings here, they're this complicated, massive shit, right? Like look at the position and bettings that we have now. Look at this. This is some hand engineered bullshit here, like this e to the i'm theta where you're basically doing, uh, this is what they look like, right? It's like, it's like, effectively the gabber filter equivalent, right? It's like, we have this hand engineered, uh, nonsense where we're basically saying, okay, well, we're gonna have this complicated function that has an imaginary component and then we're gonna have different, uh, uh, frequencies for each of the positions and then okay, not only are we gonna have different frequencies, but we're gonna be interpolating between the different frequencies and depending on where we are in the dimension, we're gonna interpolate differently. Okay, and now we're also gonna say for the first n hat, we're not gonna do interpolation and then we're also going to figure out, you're gonna, we're gonna use search to figure out the optimal value for these, it's like, you know, we're kind of like 20 steps inside a rabbit hole that we don't need to get into, right? Ultimately, I think what's really gonna be the way to solve this is that we're just gonna learn them, right? The same way that these token, uh, embeddings here, right? Which here, there's only three, this is, there's only three token embeddings here because there's only three letters, right? There's, there's only three tokens in this example, there's A, B, and C, but these token embeddings are learned, right? Nobody designed this. There was nobody who said, Hey, here's the 128 dimensional factor for king and here's the 128 dimensional vector for queen, right? Those were learned over time. But for some reason, we're still doing this like complicated mess of heuristics and human design nonsense for the position embeddings, all right? That's, that's kind of where I think this is all going. I feel like if you apply the lesson of the bitter, of the bitter lesson, apply the lesson in the bitter lesson and think about how we basically got rid of these hand engineered features in the computer vision world, we're probably going to end up doing that for these position embeddings. We're probably going to figure out a way to basically learn the optimal value of these position embeddings. So we don't have to be doing this absolute crazy shit that we do right here where it's just like this incredibly complicated system of like signs and cosines with different frequencies that all kind of overlap in such a weird way that you get something like this, right? So I don't know, that's one thing to think about right where it's like maybe, maybe this is all nonsense, you know, maybe like five years from now, you know, you're going to be sitting there, maybe you're teaching some people about AI and machine learning and you know all this thing about position embeddings and rope and long rope and yarn and it actually doesn't matter, right? Much like some, those of us that did like old school computer vision, it's like, does knowing Gabber filters matter? No, does knowing the intricate details and the historical background of how we eventually got to Gabber filters matter? No, it doesn't matter because eventually we just ended up learning them via compute and scale, right? So I think it might be the same case with us where all this weird shit that we're doing here, it's like none of it's going to matter and in five years we're just going to learn it and it's like, why is the position embedding for token one this and we're like, I don't know, that's just kind of what we got when we learned it with 10 billion trillion tokens of text. So I don't know, that's just my opinion, but learned PEs are better and there are people that have tried to do this. So there's already people doing this and they mentioned this here. So for example here, in this paper, no, this one here. So in the original rope paper, row former, they say that they're like on one side, generated absolute position and coatings, which are hand designed, was added to conceptual representations while a trainable absolute position encoding was also tried. So look at all these papers here. So look at, look at all these papers that are cited here, GenRing 2017, Devlin 2019, Lawn 2020, Clark 2020, Radford 2019, Radford 2018. So all of these papers, all of these people knew the same thing that I just described to you. They came to the same conclusion. They were like, wait a second, why are we using generated absolute position and coatings? We should train these. These should be learnable. So they basically came to the same conclusion that I just came to there and they tried to do it, but they just couldn't do it, right? The reason we're still using rope and long rope and all these things is because all these papers failed and learned position and coatings just don't work quite as well as hand designed position and coating. But I think that's just a matter of scale, right? I think the lesson from the bitter lesson is it might not work now, but eventually we're going to 10X either the data or the amount of training or the size of these things and eventually it will work. So don't lose hope. And eventually there will be a paper that follows all of these papers here that eventually says, hey, we found learnable position embeddings that outperform every heuristically hand designed positional embedding. Okay. Let's see. 

Features, feature learners. You guys are popping off here. It seems like you guys are arguing. I don't know. My, I think, like the bitter lesson is that the computation and scale wins every time, right? But there's points in time at which the bitter lesson is wrong. You know, like here whenever we were designing GABER filters, the bitter lesson was technically wrong, right? If someone came up to the to someone in like a 1900s or 1990s computer vision conference and said, hey, we should be learning these instead of hand designing these. The people would be like, haha, what a fool. If you hand design them, they're going to perform much better. And the guy was like, oh, shit, I guess, I guess you're right. I guess, I guess we should hand design these. But then 20 years later, the person who was like, hey, we should learn these. They ended up being correct. Right? So it's the same kind of thing. The hand design ones are working now, right? But eventually, at some point, we're going to get to that level of scale where we're just going to be able to learn them and the learned ones, I think, or I suspect, are probably going to be way better. But that's it guys. Let me sip this coffee, ask any questions, and then we'll summarize. 

Okay. Let's start with something. Let me give me a second to organize my thoughts. 

Okay. So today, we did a stream on long rope. So long rope is a paper that is gaining some popularity because people think that it might be behind the Gemini. So we don't really know what they did. Gemini 1.5 can't be achieved to that long context length without some new innovations. There is some secret sauce in Gemini that is allowing it to basically have very long context and good old Demis Hassab is here. You know, he's very soft, spoken man, but he's not going to tell you, you know, he doesn't give a shit about you. He doesn't give a shit about being open. And he's like, yeah, we got some secret sauce. We got some position embeddings that you don't know about. And we're not going to tell you how we did it. So people are saying, okay, well, maybe it's some kind of thing like this long rope. And I think that's why this paper was popular, and that's why a bunch of people voted this paper. So that's why we looked at it. And this is a paper from Microsoft Research Longrope is a remix of position interpolated rotary possession embeddings, which is a remix of this paper, row former. So row former is the original rope paper. This is a Chinese company. And in this paper, they basically come to this conclusion that they want, we need to add positional information to a to the tokens in a sequence that is being given to a transformer that have this inter token dependency with increasing relative distances. So what they're basically saying is, okay, we want vectors that represent the semantic meaning of each of the tokens. But then we also want these position embeddings or basically these vectors that represent the positional information of these tokens. And we also want those vectors or those position embeddings to be such that tokens that are close together have more relatedness or higher attention than tokens that are further apart. Right. So there's this kind of like things that are together in the sequence have more ability to kind of dot product in a meaningful way. Okay. So in the original paper, the intuition comes from a rotation matrix. Right. So basically, the original, the people who came up with this idea and like went through and like thought about this relative positional rope embeddings, the way that they were really thinking about it is they were basically thinking about this as a rotation matrix. Right. So a rotation matrix that in this case is rotating these vectors. Right. So this vector that represents the word enhanced, this vector that represents the word transformer. Right. You can think of it like this. Right. There's vectors that represent the words or here, right. The word vectors or the semantic vectors are these ones. And we're going to instead do this type of position embedding where you're basically almost rotating that, right. So you're coming up with this position dependent rotation matrix that is rotating those vectors such that the words that are closer together end up having a bigger dot product, right. Where that dot product is basically at the core of the attention computation, right. Where the dot product is basically this. So that's where the intuition comes from. Then we looked at this paper here. This is the meta paper where they extend rope embeddings with this idea of position interpolation. But the reason we looked at this paper is because they have a much more condensed kind of mathematical formulation that shows you how really what you're doing is you're basically creating this function f which consumes query vectors and key vectors. And when you take the real part of the dot product between the query and the keys, which is what's happening in your attention computation, what you're doing is you're effectively modifying that based on this cosine and the sine term where the cosine and the sine term are dependent on m minus n where m is the relative position of one versus the other, right. So if m minus n, if those two tokens are very close, this m minus n is going to be very small. If the tokens are very far away, this m minus n is going to be very big. And the way that they compress this with math is using the fact that Euler's identity, right, e to the i pi minus one, or just the general intuition here is that anytime you see e to the i something, especially theta's, right. Really, you could think of it like cosines and signs and I kind of showed some examples of that. But basically what another way to imagine rotary position embeddings is it's basically rotation matrices where the magnitude of the rotation is like this corkscrew in a high dimensional space, which is I guess impossible to visualize, but that's really what's going on here, right. Is your rotating these vectors based on values where the values have low frequency here, higher frequency here, and those values are such that the rotations whenever you basically have the relative position, the rotates around enough that the things that are close together in that sequence end up being more related. I feel like that was a mouthful in a half, so I'm sorry about that. But what it ends up looking like is you end up basically with this, you end up with a bunch of vectors here, you have vectors, this is the showing you what the vectors look like, right. And they kind of like there's obviously a pattern here and that pattern is coming from the fact that it's basically just a bunch of signs and cosines with different theta's. Okay, so that's what rope is, rotary position embeddings. Now we finally get to this paper, which is the paper that we read long rope, and in long rope they basically 

extend this idea of rotary position embeddings by saying, okay, well, if we have a model that's been trained for a specific context length, and we want to now use it at a longer context length, we're going to have to interpolate or rescale these position embeddings, right. So rather than having this vector of position embeddings here, which is corresponds to basically one column here, we're going to have to have more of these, we're going to have to basically increase the number of these columns because we're going to have to have more position embeddings because we're going to have to have a longer context. We're going to have to have more little empty squares here, and you have one of these columns for each empty squares, you're going to have to add more of these. So they say, okay, there's a bunch of different ways of doing this. The naive way is extrapolating which nobody does because it sucks. Meta came up with this idea of interpolating, so you basically, the value of the last one is going to be very similar to the value of the last one before, but now you're basically sampling them twice as often. But the problem with that is that you end up kind of crowding them. So the intuition there is that the rotations that you're doing are basically smaller. So now you're not rotating them by as much, so it's almost becoming a little bit more confusing, right. So the difference between this position and this position is a bigger rotation than the difference between this position and this position. So it's not quite there, right. Not quite my tempo. Okay. We have these other papers, such as yarn and NTK, where they do some fancy shit. They basically say, okay, we don't interpolate for some initial N-half positions, and also, depending on the frequency of the position embedding, which is basically how far along you are down the position embedding, right, because the position embedding frequency, you can kind of basically think of it as this part here, this N theta D, where theta i is the rotation frequency, right. So basically, as you go deeper and deeper, this is the lower dimension, higher dimension, as you go deeper and deeper into this vector, right, the frequency gets higher and higher and higher. And in this yarn paper, they say, okay, the way we interpolate at the low frequencies is going to be different than the way we interpolate at the high frequencies. So we're going to come up with some arbitrary thresholds, some non-linear point, where we're going to say, okay, here, we're going to interpolate these this way, and then we're going to interpolate these this other way, and then we're going to interpolate these this other way, right. And in this paper, what they're basically saying is, okay, well, those random arbitrary frequency-based groups that they picked were just based on human-led empirical experiments. That seems kind of arbitrary. So instead, what we're going to do is we're going to search for that. We're going to do a evolutionary search through the space of rope rescale factors and starting token enhance. So we're basically going to be fine-tuning a bunch of llama twos with slightly longer contexts, and we're going to be figuring out using the perplexity, which values for Enhat work well, which values for Lambda i work well, and we run that evolutionary search over and over and over and over again, right. And an evolutionary search is basically where you take the good performing ones and have them be parents, aka, they spawn basically variants that are just a little bit different than the parents, and then the low performing ones get thrown away, right. And then if you run this enough times, you eventually end up with a pool that is telling you, okay, here is all the really good values for Lambda i and really good values for Enhat. So you do this over and over and over and over and over again, and they do this on a bunch of GPUs, they do this on 8A116A100s, and they're doing this using red pajama, which, according to one of our viewers, is basically this giant blurb of text, and they do this and they eventually end up with basically really good values for those. And then the rest of this paper is basically just proving that what they did was good, so they show you that the long-rope mistral and long-rope llama, which are basically versions of llama and mistral that have these long-rope position embeddings, as opposed to just the standard rope position embeddings, and they show you that you have low perplexity at very high context, which is what you want. And then they also show you that they have high pass key retrieval accuracy at very high context. So basically, pass key retrieval accuracy comes from this here, right. So you can think of the pass key prompt, basically looks like this, you have a bunch of text, blah, blah, blah, blah, blah, text, text, text, text, and then randomly in the middle they say the pass key is 178065. Remember it, 178065 is the pass key, and then blah, blah, blah, more text. And then at the very, very end they say what is the pass key, right. So if you just naively take a llama 27B and use whatever extended position embeddings to extend it down to 24, like as soon as you go past the 8K context, it's basically shit. It basically never finds that. It forgets about that information. But with this long-rope embeddings, it actually does remember it, and you can see how the accuracy stays at 100. So that's the pass key retrieval accuracy. So now we know, okay, well, this long rope, it does make it better at this needle in the haystack, and you do get better perplexity. But how does it, does it, does it become bad at the other benchmarks? So does it lose anything in the MMLU and hella swag and truthful QA? Because if it loses performance there, then we're fucked, right. What's the point? Right. You have a huge context model, but it sucks at math. You know, that's not very good. So they show you that no, you know, it fails a little bit, but it's, it's not too bad. Here you go. Lama 27B with a context window of 4K, gets an arc score of 53, the fine-tuned Lama 27B with this long-rope position embeddings gets a score of 20, with a context length of 2048, gets a score of 51. So you're losing a little bit of performance, but it's still pretty good. You know, you're still getting kind of about the same performance, but now you have a significantly longer context. And hey, you didn't even have to change anything, right? So you retained the original architecture and you didn't even have to fine-tune this. You can get up to eight times context window extension without fine-tuning. And if you fine-tune, you can get to like monstrous context windows such as 2048K. So you know what? For kind of like a free lunch, this is pretty good. And that's basically the paper is basically like, as I said before, a remix on a remix on a remix of a heuristically hand-designed human thing, four position embeddings which are effectively just telling your transformer where in the sequence you're getting this information. Okay, so that was the paper. And then at the very end of the stream, I kind of made my own conclusions and prophetic predictions that I feel like all of this is going to go away. And I feel like the reason all of this is going to go away is that we're already seeing people that have been trying to basically rather than hard-code these position encodings and using all these clever signs and co-signs and different frequencies of signs and co-signs. Eventually, we're going to move away from that. Eventually, we're going to basically just learn these, right? The same way that we learned everything else. And this is kind of the rich, sought-and-bitter lesson where he's telling you rather than these hard-coded, human-designed things, right, that get more and more complicated and they just, and everybody keeps extending them and remixing them to make them even more complicated, right? So that type of expanding, heuristically designed complexity doesn't work as well as just kind of learning and scaling, right? And much like in the early days of computer vision, we were using these hand-designed Gabber filters. Eventually, we stopped doing that and we used learned features instead, right? We just let the networks tell us what the optimal convolutional filters were. So I think something similar will happen where we'll stop basically hand designing these position embeddings and we'll eventually just learn them, right? And I wouldn't be surprised if you end up with something similar, right? So the same way that when you look at Gabber filters and then you look at what comes in a confnet, it kind of almost looks like Gabber filters. So humans are pretty good at guessing, right? So I'm pretty sure that, I'm not pretty sure, but the same way that these Gabber filters kind of ended up appearing anyways, I feel like it's going to be probably similar that when we look back, the position embeddings that will end up learning will probably end up looking kind of like this, right? They'll probably end up having this kind of like sinusoidal kind of like repeating different frequencies. They'll probably be something similar, but it won't be this giant mess of shit anymore, right? It'll be kind of just vectors that are learned and that's it. So this paper in a way represents kind of the crowning achievement, the highest level of complexity that we have humans have been able to get where we're hand designing this thing. And here you have like the ultimate version of hand designing where you're taking something that was already hand design and then something that was as remix of that hand design thing and then another remix of that hand design thing. And then you're searching through the hyperparameter space of the different values in those hand design things in order to find the optimal values for the hand design thing. So 

this is like the maximum complexity. I don't think maybe not. Maybe somebody comes up with an even more hardcore version of this that has like one little extra thing stapled onto it, but this is already kind of getting way too intense IMO. And that's basically it. That's position embeddings, that's rotary position embeddings, that's long rope, and that's potentially what Google's doing with Gemini. And maybe Google's already maybe Google learned it. Maybe Google learned position embeddings and that's why Gemini is better. Or maybe they just got some 500 IQ like deepbind researcher to come up with an even more convoluted complicated version of long rope, and that's what Gemini is using. But that would be kind of lame if that's what they did. Okay. 

How does it compare to Mamba question from Hamza? So this is different from Mamba, right? In a transformer, the transformer is doing the attention between all the sequences, all the tokens in the sequence, and all and themselves every time, right? So the attention map and a transformer basically does all the computation at once. In a state space model, which is a Mamba, it's more like you're basically computing an answer and then passing it forward and then computing an answer and then passing it forward and then computing an answer and passing it forward. So like it's kind of like a recurrent neural network where it's like the position information or the fact that what this thing happened X times ago and X steps ago is like almost more within the implicit bias of, so a state space model has an implicit bias of kind of sequence and position versus a transformer does not have that implicit bias. The transformer does not have an implicit bias of like this is first, this is second, this is third, this is fourth, right? So in a transformer, you have to add these position embeddings, but in a state space model, you don't need to do that because it's more, you could probably do that. I bet you there's probably state space models or Mamba's that have position information in there explicitly, but it's kind of more like the sequential nature of it is kind of inherent in the state space architecture. Okay. Transformer is still the strongest architecture. Yeah, I agree with that. No, my GF, my GF is shocked about the evolutionary strategy. It's a weird thing to try to explain to her. Okay. How much VRAM do you need for one million tokens a lot? Probably. Microsoft sharing this paper basically after Gemini. Yeah, I mean, Microsoft doesn't know either, right? It's like, there's probably some people at Microsoft who kind of know what's happening at Gemini. Maybe, maybe Microsoft poaches some people and then Google poaches other people. So there's probably enough kind of like horizontal transfer of knowledge within the big companies that they kind of know what the other people are doing, but they're going to actively try to prevent that, right? So that was more so the case like 10 years ago. I think that now that these companies are increasingly more and more secretive, you might end up with compartmentalization, right? And the compartmentalization means that the engineers that are working on these type of things don't actually know what the other engineers are working on so that if they ever get poached, they don't actually know what the other thing is, right? And I think this is already probably pretty rampant. One thing that kind of made me think about that is that if you look at the Carpati tokenization video, there's a lot of points where he's like, I think we're doing this. I don't know if we're doing this. I don't know if we're doing that, right? It's like, and I thought that was weird. I was like, why shouldn't you Carpati know everything that's happening at OpenAI? Like shouldn't you be aware of all the different things that are being done for GPD4? Like, how could it be that you don't actually know what is being done? And I think the reason for that is that Sam Altman, right? Think of Sam Altman going and watching Oppenheimer coming out of Oppenheimer and being like, we should compartmentalize our stuff. And he's sitting there coming up with scenarios. He's like, what if Carpati leaves? Dude, if Carpati leaves and goes to Microsoft and goes to Google, we're fucked because he knows everything. So therefore, the answer is to compartmentalize and not tell Carpati anything. So Carpati basically doesn't know what the fuck is going on so that if someone poaches him, he doesn't actually know what we're using for the tokenizer. He doesn't actually know the data set that we're using, right? Because he's only allowed to know this very limited thing. So it could be that type of situation, right? Where the people who work at Google who actually know the position embeddings that are being used to give Gemini a 10 million context or whatever the context like this, they're basically insulated from the other people at DeepMind so that if you were to poach someone else, that someone else won't necessarily know that. And I mean, I think that's terrible. I don't like this idea of compartmentalization. I don't like the idea of secrecy. I think both of those are just like anti-human. You know, I feel like humanity needs to move together. Like I feel like I'm all about sharing, which is why I literally talk about papers on the internet, but like that's probably what's happening behind the scenes. This like kind of excessive compartmentalization that and secrecy that prevents this kind of horizontal transfer via poaching. 

Okay. Any other questions here? 

How do they create better children from good parents? You don't know, right? So in an evolutionary search, you the children are created from the parents, but it's not necessarily guaranteed that the children of the parents are going to be better than the parents, right? So the assumption behind evolution is that if the parents are high performing and the environment stays relatively consistent, then the children of the parents that are basically just not very far away from the parents in terms of like the parameter space, right? They're probably going to be just as good or just a little bit worse or just a little bit better, but it's like here, the environment is the same, right? So it's like you're evaluating the same thing every time. You're you're basically giving it the same data set, fine tuning on the same thing, and then getting the perplexity. So there's no guarantee that the children are going to be better than the parents, but on average, the children of good parents are going to be way better than the children of bad parents, and that's how evolution works and kind of gets better over time. 

Question from Kareem. The right inductive biases typically do show improved performance. Yes, and that's kind of, I would say that goes back to the bitter lesson, right? You want inductive biases, inductive biases that you come up with as a human, right? Such as the inductive bias here of saying, hey, we want kind of positional information. So therefore, using signs and cosines where you go up and down and up, like that's probably a good inductive bias, right? This kind of like repeating periodic functions. That's probably a good inductive bias for position embeddings, but yes, they work well, but the bitter lesson is that over time, you're actually shooting yourself in the foot. You're better off just trying to put as little inductive biases as possible and just let learning and scale just find the best solution for you, right? So that's also why the transformer is so powerful, right? The transformer is one of the architectures that has the least amount of biases, right? So I think it's no coincidence that the best architecture that we have right now is the one with the least inductive biases, even though you could make the argument that a good inductive bias is is is that benefit is a benefit? Ultimately, I think that no inductive bias is kind of wins out. Just random jitter plus noise plus brute force kind of. Do you think Google and Microsoft are using non-compete contracts? So non-compete clauses are not really enforceable in California. So California has unique laws around that basically where it's like if you work at a company and do something and then go do a startup that is basically the same thing in a lot of places, you can the the original company that you were working for can sue you and say, hey, that's a non-compete, right? You can't be doing that because you're basically competing with us and we're the ones that taught you that. But in California, it's very difficult to do that. It's very difficult to enforce these kind of non-compete, which means that it's very easy to poach people, right? So like if you're in California and you have a robotics company and you want to basically compete with the other robotics company, you can just hire all their people and have them come to your company and just do what they were doing over there over here. And that other company is going to have a very, very hard time suing you under a non-compete kind of arguments, which is why you get all this kind of like back and forth bullshit where like, you know, Elon's company is like he's he does this a lot. He poaches people constantly, you know, like if you look at the people working on the optimist robot, that's all fucking Google people. It's like he just poached all of them, right? So the argument is that it's good for progress, you know, you don't want non-compete. If it was very difficult to get people to work on your thing if they had already worked on that in a different company, then basically progress would be much slower. So I think the fact that non-compete are not really enforceable is a good thing for humanity. I don't know, but that's just me. I'm like of knowledge, free knowledge for everybody, everybody share everything kind of kumbaya kind of guy, but, you know, some people just want to make money. Some people just want to get rich and they're okay of humanity's progress is a little bit slower, but they get rich. Okay. That's all I got guys. I'm tired. My voice is running out. Hopefully you guys got something out of that. I think we touched on a bunch of different points. See you next week, I guess. Don't know what I'm going to stream on, but I know all stream. See you guys later. Thanks. Humsha, Christopher. Slappy to Frog, Jeha, Aries, Raphael, Humsha, 87 GN, Noah, Emily, ML Cat, Khalil, Christopher again, Pratush, Karim, Khalil, the lookerful for the for the super chat for the money, I guess, Spyro, who else? Majeddy, Wehan, Firaus, Videk, Genki, Sam. At some point, there's going to be so many people that it's going to be impossible for me to call out everybody, but I'll try to keep doing this until then. Mustafa, Pineapple, and 87. Thanks everybody for listening. Hope you got something out of that and have a great day. 
